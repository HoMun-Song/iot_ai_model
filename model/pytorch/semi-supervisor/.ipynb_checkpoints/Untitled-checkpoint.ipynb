{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8965cc6e-b3a8-4a7d-8a22-4503363359d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from datetime import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99eacba4-0128-4676-84eb-da0b9c3a782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 인코더\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size=4096, hidden_size=1024, num_layers=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True,\n",
    "        #                     dropout=0.1, bidirectional=False)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                            batch_first=True, dropout=0.1, bidirectional=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, hidden = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        return hidden\n",
    "    \n",
    "## 디코더\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size=4096, hidden_size=1024, output_size=4096, num_layers=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "#         self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True,\n",
    "#                             dropout=0.1, bidirectional=False)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                            batch_first=True, dropout=0.1, bidirectional=False)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.lstm(x, hidden)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        prediction = self.fc(output)\n",
    "\n",
    "        return prediction, hidden\n",
    "    \n",
    "## RNN Auto Encoder\n",
    "class RNNAutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 latent_dim: int,\n",
    "                 window_size: int=1,\n",
    "                 **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        :param input_dim: 변수 Tag 갯수\n",
    "        :param latent_dim: 최종 압축할 차원 크기\n",
    "        :param window_size: 길이\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "\n",
    "        super(RNNAutoEncoder, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.window_size = window_size\n",
    "\n",
    "        if \"num_layers\" in kwargs:\n",
    "            num_layers = kwargs.pop(\"num_layers\")\n",
    "        else:\n",
    "            num_layers = 1\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=latent_dim,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.reconstruct_decoder = Decoder(\n",
    "            input_size=input_dim,\n",
    "            output_size=input_dim,\n",
    "            hidden_size=latent_dim,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, src:torch.Tensor, **kwargs):\n",
    "        batch_size, sequence_length, var_length = src.size()\n",
    "\n",
    "        ## Encoder 넣기\n",
    "        encoder_hidden = self.encoder(src)\n",
    "        \n",
    "        inv_idx = torch.arange(sequence_length - 1, -1, -1).long()\n",
    "        reconstruct_output = []\n",
    "        temp_input = torch.zeros((batch_size, 1, var_length), dtype=torch.float).to(src.device)\n",
    "        hidden = encoder_hidden\n",
    "        for t in range(sequence_length):\n",
    "            temp_input, hidden = self.reconstruct_decoder(temp_input, hidden)\n",
    "            reconstruct_output.append(temp_input)\n",
    "        reconstruct_output = torch.cat(reconstruct_output, dim=1)[:, inv_idx, :]\n",
    "        \n",
    "        return [reconstruct_output, src]\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        \n",
    "        ## MSE loss(Mean squared Error)\n",
    "        loss =F.mse_loss(recons, input)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bff4282a-8fd5-406a-8236-0f87a83f281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CurrentDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33eeb304-fe0c-4478-8ebe-00776ba860b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/workspace/iot_ai_model/dataset/current/train/**/normal/*.csv'\n",
    "dataset = CurrentDataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbe34cc1-a539-4286-a3f4-02003638f3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19007823-9704-4034-8f28-4b58d22c0ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = RNNAutoEncoder(3, 500, num_layers=2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e604e5e-d629-4cf4-8012-a7306956b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dataloader = DataLoader(dataset,\n",
    "                            batch_size=128,\n",
    "                            shuffle=True,\n",
    "                            num_workers=2,\n",
    "                            pin_memory=True,\n",
    "                            drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d855a91c-9d70-4e21-a33e-a261e7c48e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f558dbfb-30a3-426b-a7a8-b38c9a7ec5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 265/533 [05:18<05:21,  1.20s/it, loss 117.00360107421875]"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    progress = tqdm(trn_dataloader)\n",
    "    for path, cur, target in progress:\n",
    "        cur = cur.cuda()\n",
    "        output, _ = model(cur)\n",
    "        loss = criterion(cur, output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress.set_postfix_str(f'loss {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46376dac-c29f-4daf-ab24-089cbf402ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2000, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae0107-cbf3-4b7f-a479-1c051acf9b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)\n",
    "rnn.train()\n",
    "hist_loss = []\n",
    "pre_loss = 1.\n",
    "early_stopping_count = 0\n",
    "for epoch in range(epochs):\n",
    "    cost = .0 \n",
    "    for x in tqdm(train_loader):\n",
    "        pred,_ = model(x)\n",
    "\n",
    "        ## MSE loss(Mean squared Error) loss를 사용\n",
    "        loss = criterion(x, pred)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        cost += loss.item()\n",
    "    avg_loss = cost / len(train_loader)\n",
    "    hist_loss.append(avg_loss)    \n",
    "    print('epoch:', epoch, 'loss:', avg_loss)\n",
    "\n",
    "    # early_stopping. 학습 중단 조건 코드\n",
    "    if epoch > 0 and hist_loss[epoch - 1] > avg_loss:\n",
    "        early_stopping_count = 0\n",
    "        torch.save(rnn.state_dict(), f'/content/drive/MyDrive/colab/cloudflow/model_state_{epoch}.pt')\n",
    "    else:\n",
    "        early_stopping_count += 1\n",
    "    if early_stopping_count >=3 :\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
